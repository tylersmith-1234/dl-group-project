{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/yichen/anaconda3/lib/python3.9/site-packages (4.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: sacremoses in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: requests in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: joblib in /Users/yichen/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/yichen/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/yichen/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: adapter-transformers in /Users/yichen/anaconda3/lib/python3.9/site-packages (3.0.0)\n",
      "Requirement already satisfied: requests in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (0.12.1)\n",
      "Requirement already satisfied: sacremoses in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (0.0.49)\n",
      "Requirement already satisfied: filelock in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (2021.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from adapter-transformers) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->adapter-transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->adapter-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->adapter-transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->adapter-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->adapter-transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->adapter-transformers) (1.26.7)\n",
      "Requirement already satisfied: six in /Users/yichen/anaconda3/lib/python3.9/site-packages (from sacremoses->adapter-transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/yichen/anaconda3/lib/python3.9/site-packages (from sacremoses->adapter-transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/yichen/anaconda3/lib/python3.9/site-packages (from sacremoses->adapter-transformers) (8.0.3)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets in /Users/yichen/anaconda3/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (2021.8.1)\n",
      "Requirement already satisfied: dill in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: packaging in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: xxhash in /Users/yichen/anaconda3/lib/python3.9/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/yichen/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/yichen/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytorch_lightning in /Users/yichen/anaconda3/lib/python3.9/site-packages (1.6.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (2021.8.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.8.1)\n",
      "Requirement already satisfied: torch>=1.8.* in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.11.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.20.3)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (0.3.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: requests in /Users/yichen/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in /Users/yichen/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.44.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (58.0.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.6.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.20.1)\n",
      "Requirement already satisfied: six in /Users/yichen/anaconda3/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yichen/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.2.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in /Users/yichen/anaconda3/lib/python3.9/site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install -U adapter-transformers\n",
    "!pip install datasets\n",
    "!pip install pytorch_lightning\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import datasets\n",
    "\n",
    "from transformers import (\n",
    "    # AdamW, \n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    T5AdapterModel, \n",
    "    T5Tokenizer, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    TrainingArguments, \n",
    "    AdapterTrainer,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from dataprovider.DataProvider import DatasetProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PCCW's chief operating officer, Mike Butcher, ...</td>\n",
       "      <td>Current Chief Operating Officer Mike Butcher a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The world's two largest automakers said their ...</td>\n",
       "      <td>Domestic sales at both GM and No. 2 Ford Motor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>According to the federal Centers for Disease C...</td>\n",
       "      <td>The Centers for Disease Control and Prevention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A tropical storm rapidly developed in the Gulf...</td>\n",
       "      <td>A tropical storm rapidly developed in the Gulf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The company didn't detail the costs of the rep...</td>\n",
       "      <td>But company officials expect the costs of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>Twice Sparrow sold the island twice to Thomas ...</td>\n",
       "      <td>Sparrow twice sold the island to Thomas Polloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>The name in Tupi means `` insensitive stone ''...</td>\n",
       "      <td>The name in Tupi means '' hard stone `` , '' i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>The company has branches in Tokyo , based in t...</td>\n",
       "      <td>The company has branches in Tokyo based in Sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>The modern coat of arms of Bavaria was designe...</td>\n",
       "      <td>The modern coat of arms of Bavaria was designe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>It is located near Point Pleasant Borough , a ...</td>\n",
       "      <td>It is near Point Pleasant borough , a municipa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5177 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     in  \\\n",
       "0     PCCW's chief operating officer, Mike Butcher, ...   \n",
       "1     The world's two largest automakers said their ...   \n",
       "2     According to the federal Centers for Disease C...   \n",
       "3     A tropical storm rapidly developed in the Gulf...   \n",
       "4     The company didn't detail the costs of the rep...   \n",
       "...                                                 ...   \n",
       "5172  Twice Sparrow sold the island twice to Thomas ...   \n",
       "5173  The name in Tupi means `` insensitive stone ''...   \n",
       "5174  The company has branches in Tokyo , based in t...   \n",
       "5175  The modern coat of arms of Bavaria was designe...   \n",
       "5176  It is located near Point Pleasant Borough , a ...   \n",
       "\n",
       "                                               expected  \n",
       "0     Current Chief Operating Officer Mike Butcher a...  \n",
       "1     Domestic sales at both GM and No. 2 Ford Motor...  \n",
       "2     The Centers for Disease Control and Prevention...  \n",
       "3     A tropical storm rapidly developed in the Gulf...  \n",
       "4     But company officials expect the costs of the ...  \n",
       "...                                                 ...  \n",
       "5172  Sparrow twice sold the island to Thomas Polloc...  \n",
       "5173  The name in Tupi means '' hard stone `` , '' i...  \n",
       "5174  The company has branches in Tokyo based in Sai...  \n",
       "5175  The modern coat of arms of Bavaria was designe...  \n",
       "5176  It is near Point Pleasant borough , a municipa...  \n",
       "\n",
       "[5177 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/COMBINED/test.tsv', sep = '\\t', names=['in', 'expected'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in</th>\n",
       "      <th>expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58613</th>\n",
       "      <td>Tommy Connolly , who plays Rory Jennings , pla...</td>\n",
       "      <td>Tommy Connolly , who plays Rory Jennings , pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58614</th>\n",
       "      <td>Monroe Meadows , in Yosemite valley near Brida...</td>\n",
       "      <td>Monroe Meadows , in Yosemite Valley near Brida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58615</th>\n",
       "      <td>Monroe Meadows , in Yosemite Valley near Brida...</td>\n",
       "      <td>Monroe Meadows , in Yosemite valley near Brida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58616</th>\n",
       "      <td>In 2014 the site launched iOS and Android appl...</td>\n",
       "      <td>In 2014 launched the site iOS and Android - ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58617</th>\n",
       "      <td>In 2014 launched the site iOS and Android - ap...</td>\n",
       "      <td>In 2014 the site launched iOS and Android appl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58618 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      in  \\\n",
       "0      Amrozi accused his brother, whom he called \"th...   \n",
       "1      Referring to him as only \"the witness\", Amrozi...   \n",
       "2      Yucaipa owned Dominick's before selling the ch...   \n",
       "3      Yucaipa bought Dominick's in 1995 for $693 mil...   \n",
       "4      They had published an advertisement on the Int...   \n",
       "...                                                  ...   \n",
       "58613  Tommy Connolly , who plays Rory Jennings , pla...   \n",
       "58614  Monroe Meadows , in Yosemite valley near Brida...   \n",
       "58615  Monroe Meadows , in Yosemite Valley near Brida...   \n",
       "58616  In 2014 the site launched iOS and Android appl...   \n",
       "58617  In 2014 launched the site iOS and Android - ap...   \n",
       "\n",
       "                                                expected  \n",
       "0      Referring to him as only \"the witness\", Amrozi...  \n",
       "1      Amrozi accused his brother, whom he called \"th...  \n",
       "2      Yucaipa bought Dominick's in 1995 for $693 mil...  \n",
       "3      Yucaipa owned Dominick's before selling the ch...  \n",
       "4      On June 10, the ship's owners had published an...  \n",
       "...                                                  ...  \n",
       "58613  Tommy Connolly , who plays Rory Jennings , pla...  \n",
       "58614  Monroe Meadows , in Yosemite Valley near Brida...  \n",
       "58615  Monroe Meadows , in Yosemite valley near Brida...  \n",
       "58616  In 2014 launched the site iOS and Android - ap...  \n",
       "58617  In 2014 the site launched iOS and Android appl...  \n",
       "\n",
       "[58618 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/COMBINED/train.tsv', sep = '\\t', names=['in', 'expected'])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/yichen/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /Users/yichen/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /Users/yichen/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/yichen/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "  0%|          | 0/11250 [12:15<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "100%|██████████| 6/6 [00:00<00:00, 10.91ba/s]\n",
      "100%|██████████| 59/59 [00:05<00:00, 10.84ba/s]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "\n",
    "###\n",
    "init_weights = 't5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_weights)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prefix = 'paraphrase: '\n",
    "    inputs = [prefix + doc for doc in examples[\"in\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"expected\"], max_length=512, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test = dataset_test.map(preprocess_function, batched=True)\n",
    "tokenized_train = dataset_train.map(preprocess_function, batched=True)\n",
    "\n",
    "###\n",
    "num_train = 30000\n",
    "num_test = 1500\n",
    "tokenized_train_small = tokenized_train.shuffle(seed=42).select(range(100))\n",
    "tokenized_test_small = tokenized_test.shuffle(seed=42).select(range(10))\n",
    "\n",
    "###\n",
    "tokenized_train_small.set_format(type=\"torch\", columns=[\"attention_mask\", \"input_ids\", \"labels\"],)\n",
    "tokenized_test_small.set_format(type=\"torch\", columns=[\"attention_mask\", \"input_ids\", \"labels\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapter Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /Users/yichen/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /Users/yichen/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "Some weights of the model checkpoint at t5-small were not used when initializing T5AdapterModel: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
      "- This IS expected if you are initializing T5AdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5AdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5AdapterModel were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'lm_head' with config {'head_type': 'seq2seq_lm', 'vocab_size': 32128, 'layers': 1, 'activation_function': None, 'layer_norm': False, 'bias': False, 'shift_labels': False, 'label2id': None}.\n",
      "Adding adapter 'paraphrase1'.\n"
     ]
    }
   ],
   "source": [
    "adapter_model = T5AdapterModel.from_pretrained(\"t5-small\")\n",
    "adapter_model.add_seq2seq_lm_head(\"lm_head\")\n",
    "adapter_model.add_adapter(\"paraphrase1\")\n",
    "adapter_model.train_adapter(\"paraphrase1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=adapter_model)\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=adapter_model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_small,\n",
    "    eval_dataset=tokenized_test_small,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n",
      " 33%|███▎      | 13/39 [00:16<00:27,  1.05s/it]***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      " 33%|███▎      | 13/39 [00:16<00:27,  1.05s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1636545658111572, 'eval_runtime': 0.8603, 'eval_samples_per_second': 11.624, 'eval_steps_per_second': 2.325, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 26/39 [00:33<00:14,  1.12s/it]***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                  \n",
      "\n",
      "                                               \n",
      " 67%|██████▋   | 26/39 [00:33<00:14,  1.12s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1636545658111572, 'eval_runtime': 0.8448, 'eval_samples_per_second': 11.838, 'eval_steps_per_second': 2.368, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:50<00:00,  1.10s/it]***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                  \n",
      "\n",
      "                                               \n",
      "100%|██████████| 39/39 [00:51<00:00,  1.10s/it]\n",
      "\u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 39/39 [00:51<00:00,  1.32s/it]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1636545658111572, 'eval_runtime': 0.8876, 'eval_samples_per_second': 11.266, 'eval_steps_per_second': 2.253, 'epoch': 3.0}\n",
      "{'train_runtime': 51.4115, 'train_samples_per_second': 5.835, 'train_steps_per_second': 0.759, 'train_loss': 0.9082233722393329, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=0.9082233722393329, metrics={'train_runtime': 51.4115, 'train_samples_per_second': 5.835, 'train_steps_per_second': 0.759, 'train_loss': 0.9082233722393329, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in adapter_paraphrase/adapter_config.json\n",
      "Module weights saved in adapter_paraphrase/pytorch_adapter.bin\n"
     ]
    }
   ],
   "source": [
    "adapter_model.save_adapter(\"adapter_paraphrase\", \"paraphrase1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_model.save_pretrained('results/checkpoint/t5adapter-save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/checkpoint/adapter_paraphrase1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k0/87xrhg8j4fxfkpc2lcsf1n400000gn/T/ipykernel_46845/1654373947.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madapter_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results/checkpoint/adapter_paraphrase1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"paraphrase1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/adapters/model_mixin.py\u001b[0m in \u001b[0;36msave_adapter\u001b[0;34m(self, save_directory, adapter_name, with_head, meta_dict, custom_weights_loaders)\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mcustom_weights_loaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcustom_weights_loaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPredictionHeadLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_on_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         super().save_adapter(\n\u001b[0m\u001b[1;32m    878\u001b[0m             \u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/adapters/model_mixin.py\u001b[0m in \u001b[0;36msave_adapter\u001b[0;34m(self, save_directory, adapter_name, meta_dict, custom_weights_loaders)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m    394\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdapterLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# save additional custom weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_weights_loaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/adapters/loading.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, save_directory, name, meta_dict)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             assert isdir(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/checkpoint/adapter_paraphrase1'"
     ]
    }
   ],
   "source": [
    "adapter_model.save_adapter(\"results/checkpoint/adapter_paraphrase1\", \"paraphrase1\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e23ae9c18059c14f6dbb9512cd074d0e70d1218061a550ea1ab032208b56eb7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
